{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.cPickle import load\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import XMLMultiDocPreprocessor, CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser.corenlp import StanfordCoreNLPServer\n",
    "from snorkel.models import Document, Sentence, Candidate, candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels, FeatureAnnotator, save_marginals, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression, GenerativeModel, RandomSearch\n",
    "from snorkel.learning.structure import DependencySelector\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "# from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "import matchers\n",
    "import LF\n",
    "from candidate_adjective_fixer import *\n",
    "from load_external_annotations_new import load_external_labels\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "BiomarkerType = candidate_subclass('BiomarkerType', ['biomarker', 'tipo'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------\n",
    "# Helper Functions\n",
    "#------------------\n",
    "\n",
    "def grabCandidates(extractor, schema):\n",
    "    # Candidate Counts\n",
    "    for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "        extractor.apply(sents, split=k, clear=False)\n",
    "        print \"Number of candidates: \", session.query(schema).filter(schema.split == k).count()\n",
    "        session.commit()\n",
    "        \n",
    "    train_cands = session.query(schema).filter(\n",
    "        schema.split == 0).all()\n",
    "    dev_cands = session.query(schema).filter(\n",
    "        schema.split == 1).all()\n",
    "    test_cands = session.query(schema).filter(\n",
    "        schema.split == 2).all()\n",
    "\n",
    "    return [train_cands, dev_cands, test_cands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# Setup & Preprocessing\n",
    "#-----------------------\n",
    "\n",
    "# Instantiate the Session\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Doc Preprocessing\n",
    "file_path = 'articles/training.xml'\n",
    "train_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//article',\n",
    "    text='.//front/article-meta/abstract/p/text()',\n",
    "    id='.//front/article-meta/article-id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/development.xml'\n",
    "dev_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/testcorpus.xml'\n",
    "test_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "# Parsing\n",
    "# corenlp_server = StanfordCoreNLPServer(version=\"3.6.0\", num_threads=4, port=12348)\n",
    "# corpus_parser = CorpusParser(corenlp_server, parser=Spacy())\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "# corpus_parser = CorpusParser()\n",
    "\n",
    "# Note: Parallelism can be run with a Postgres DBMS, but not SQLite\n",
    "corpus_parser.apply(list(train_preprocessor))\n",
    "corpus_parser.apply(list(dev_preprocessor), clear=False)\n",
    "corpus_parser.apply(list(test_preprocessor), clear=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving Stable IDs for each of the candidate sentences\n",
    "with open('articles/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids, test_ids = load(f)\n",
    "\n",
    "train_ids, dev_ids, test_ids = set(train_ids), set(dev_ids), set(test_ids)\n",
    "train_sents, dev_sents, test_sents = set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "\n",
    "# Assigning each sentence to {train,dev,test}-set based on Stable ID\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Candidate Extraction\n",
    "#----------------------\n",
    "\n",
    "# Defining the Candidate Schemas\n",
    "# BiomarkerCondition = candidate_subclass('BiomarkerCondition', ['biomarker', 'condition'])\n",
    "\n",
    "# BiomarkerDrug = candidate_subclass('BiomarkerDrug', ['biomarker', 'drug'])\n",
    "# BiomarkerMedium = candidate_subclass('BiomarkerMedium', ['biomarker', 'medium'])\n",
    "# BiomarkerType = candidate_subclass('BiomarkerType', ['biomarker', 'typ3'])\n",
    "# # BiomarkerLevelUnit = candidate_subclass('BiomarkerLevelUnit', ['biomarker', 'level', 'unit'])\n",
    "#can eventually add MEASUREMENT and COHORT SIZE among other entities\n",
    "\n",
    "# N-grams: the probabilistic search space of our entities\n",
    "biomarker_ngrams = Ngrams(n_max=1)\n",
    "# condition_ngrams = Ngrams(n_max=7)\n",
    "# drug_ngrams = Ngrams(n_max=5)\n",
    "# medium_ngrams = Ngrams(n_max=5)\n",
    "type_ngrams = Ngrams(n_max=5)  # <--- Q: should we cut these down?\n",
    "# # level_ngrams = Ngrams(n_max=1)\n",
    "# unit_ngrams = Ngrams(n_max=1)\n",
    "\n",
    "# Construct our Matchers\n",
    "bMatcher = matchers.getBiomarkerMatcher()\n",
    "# cMatcher = matchers.getConditionMatcher()\n",
    "# dMatcher = matchers.getDrugMatcher()\n",
    "# mMatcher = matchers.getMediumMatcher()\n",
    "tMatcher = matchers.getTypeMatcher()\n",
    "# lMatcher = matchers.getLevelMatcher()\n",
    "# uMatcher = matchers.getUnitMatcher()\n",
    "\n",
    "# Building the CandidateExtractors\n",
    "# candidate_extractor_BC = CandidateExtractor(BiomarkerCondition, [biomarker_ngrams, condition_ngrams], [bMatcher, cMatcher])\n",
    "# candidate_extractor_BD = CandidateExtractor(BiomarkerDrug, [biomarker_ngrams, drug_ngrams], [bMatcher, dMatcher])\n",
    "# candidate_extractor_BM = CandidateExtractor(BiomarkerMedium, [biomarker_ngrams, medium_ngrams], [bMatcher, mMatcher])\n",
    "candidate_extractor_BT = CandidateExtractor(BiomarkerType, [biomarker_ngrams, type_ngrams], [bMatcher, tMatcher])\n",
    "# candidate_extractor_BLU = CandidateExtractor(BiomarkerLevelUnit, [biomarker_ngrams, level_ngrams, unit_ngrams], [bMatcher, lMatcher, uMatcher])\n",
    "\n",
    "# List of Candidate Sets for each relation type: [train, dev, test]\n",
    "# cands_BC = grabCandidates(candidate_extractor_BC, BiomarkerCondition)\n",
    "# cands_BD = grabCandidates(candidate_extractor_BD, BiomarkerDrug)\n",
    "# cands_BM = grabCandidates(candidate_extractor_BM, BiomarkerMedium)\n",
    "cands_BT = grabCandidates(candidate_extractor_BT, BiomarkerType)\n",
    "# cands_BLU = grabCandidates(candidate_extractor_BLU, BiomarkerLevelUnit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF import *\n",
    "LFs_BT = [LF_colon, LF_known_abs, LF_single_letter, LF_roman_numeral, LF_common_2000, LF_same_thing, LF_distance, LF_abstract_titleWord, \n",
    "          LF_auxpass, LF_markerDatabase, LF_is_in_typeDatabase]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "BT_labeler = LabelAnnotator(lfs=LFs_BT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_train_BT = BT_labeler.apply(split=0)\n",
    "L_train_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time L_train_BT = BT_labeler.load_matrix(session, split=0)\n",
    "L_train_BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_BT.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_BT.get_key(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train_BT, epochs=100, decay=0.95, step_size=0.1 / L_train_BT.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train_BT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = BT_labeler.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train_BT, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(BiomarkerType).filter(BiomarkerType.split == 0).order_by(BiomarkerType.id).all()\n",
    "dev_cands   = session.query(BiomarkerType).filter(BiomarkerType.split == 1).order_by(BiomarkerType.id).all()\n",
    "test_cands  = session.query(BiomarkerType).filter(BiomarkerType.split == 1).order_by(BiomarkerType.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "load_external_labels(session, BiomarkerType, 'Biomarker', 'Type', 'articles/type_gold_labels.tsv', dev_cands, annotator_name='gold')\n",
    "load_external_labels(session, BiomarkerType, 'Biomarker', 'Type', 'articles/type_gold_labels.tsv', test_cands, annotator_name='gold')\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(train_cands)\n",
    "print len(dev_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        50,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.25,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 100\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = lstm.score(test_cands, L_gold_test)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, test_cands, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save_marginals(session, test_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lstm.predictions(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for prediction in predictions: \n",
    "    if(prediction == 1):\n",
    "        i+=1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while( i< len(train_cands)):\n",
    "    print(\"Candidate: {}. Prediction: {}\").format(train_cands[i], predictions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
