{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markerville Backend\n",
    "\n",
    "## Imports and Candidate Subclass\n",
    "\n",
    "All the imports and the creation of the candidate_subclass or desired relationship to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.cPickle import load\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import XMLMultiDocPreprocessor, CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.parser.corenlp import StanfordCoreNLPServer\n",
    "from snorkel.models import Document, Sentence, Candidate, candidate_subclass\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels, FeatureAnnotator, save_marginals, load_marginals\n",
    "from snorkel.learning import SparseLogisticRegression, GenerativeModel, RandomSearch\n",
    "from snorkel.learning.structure import DependencySelector\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "# from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "import matchers\n",
    "import LF\n",
    "from candidate_adjective_fixer import *\n",
    "from load_external_annotations_new import load_external_labels\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "BiomarkerCondition = candidate_subclass('BiomarkerCondition', ['biomarker', 'condition'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------\n",
    "# Helper Functions\n",
    "#------------------\n",
    "\n",
    "def grabCandidates(extractor, schema):\n",
    "    # Candidate Counts\n",
    "    for k, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "        extractor.apply(sents, split=k, clear=False)\n",
    "        print \"Number of candidates: \", session.query(schema).filter(schema.split == k).count()\n",
    "        session.commit()\n",
    "        \n",
    "    train_cands = session.query(schema).filter(\n",
    "        schema.split == 0).all()\n",
    "    dev_cands = session.query(schema).filter(\n",
    "        schema.split == 1).all()\n",
    "    test_cands = session.query(schema).filter(\n",
    "        schema.split == 2).all()\n",
    "\n",
    "    return [train_cands, dev_cands, test_cands]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Preprocessing\n",
    "\n",
    "Load in the XML files containing the corpuses. In this case, there are three, a training corpus, test corpus, and development corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# Setup & Preprocessing\n",
    "#-----------------------\n",
    "\n",
    "# Instantiate the Session\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Doc Preprocessing\n",
    "file_path = 'articles/training.xml'\n",
    "train_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//article',\n",
    "    text='.//front/article-meta/abstract/p/text()',\n",
    "    id='.//front/article-meta/article-id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/development.xml'\n",
    "dev_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "file_path = 'articles/testcorpus.xml'\n",
    "test_preprocessor = XMLMultiDocPreprocessor(\n",
    "    path=file_path,\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()'\n",
    ")\n",
    "\n",
    "# Parsing\n",
    "# corenlp_server = StanfordCoreNLPServer(version=\"3.6.0\", num_threads=4, port=12348)\n",
    "# corpus_parser = CorpusParser(corenlp_server, parser=Spacy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CorpusParser is applied to each of these corpuses to break them into Documents and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "# corpus_parser = CorpusParser()\n",
    "\n",
    "# Note: Parallelism can be run with a Postgres DBMS, but not SQLite\n",
    "corpus_parser.apply(list(train_preprocessor))\n",
    "corpus_parser.apply(list(dev_preprocessor), clear=False)\n",
    "corpus_parser.apply(list(test_preprocessor), clear=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences are then split into train, dev, and test according to the document IDs associated with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving Stable IDs for each of the candidate sentences\n",
    "with open('articles/doc_ids.pkl', 'rb') as f:\n",
    "    train_ids, dev_ids, test_ids = load(f)\n",
    "\n",
    "train_ids, dev_ids, test_ids = set(train_ids), set(dev_ids), set(test_ids)\n",
    "train_sents, dev_sents, test_sents = set(), set(), set()\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "\n",
    "# Assigning each sentence to {train,dev,test}-set based on Stable ID\n",
    "for i, doc in enumerate(docs):\n",
    "    for s in doc.sentences:\n",
    "        if doc.name in train_ids:\n",
    "            train_sents.add(s)\n",
    "        elif doc.name in dev_ids:\n",
    "            dev_sents.add(s)\n",
    "        elif doc.name in test_ids:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'ID <{0}> not found in any id set'.format(doc.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Extraction\n",
    "\n",
    "The Ngrams for each entity, or number of words to match, is established. Then the matchers (collection of regular expressions and dictionaries) are initialized. \n",
    "\n",
    "The matchers, ngrams, and candidate_subclass are passed into the CandidateExtractor to extract candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Candidate Extraction\n",
    "#----------------------\n",
    "\n",
    "# Defining the Candidate Schemas\n",
    "BiomarkerCondition = candidate_subclass('BiomarkerCondition', ['biomarker', 'condition'])\n",
    "\n",
    "# N-grams: the probabilistic search space of our entities\n",
    "biomarker_ngrams = Ngrams(n_max=1)\n",
    "condition_ngrams = Ngrams(n_max=7)\n",
    "\n",
    "# Construct our Matchers\n",
    "bMatcher = matchers.getBiomarkerMatcher()\n",
    "cMatcher = matchers.getConditionMatcher()\n",
    "\n",
    "# Building the CandidateExtractors\n",
    "candidate_extractor_BC = CandidateExtractor(BiomarkerCondition, [biomarker_ngrams, condition_ngrams], [bMatcher, cMatcher])\n",
    "\n",
    "# List of Candidate Sets for each relation type: [train, dev, test]\n",
    "cands_BC = grabCandidates(candidate_extractor_BC, BiomarkerCondition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of specificity issues, for medium, condition, and drug, grabs the adjectives in front of the entity as well. The goal is to have more specific entities, such as esophaegal cancer rather than just cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()\n",
    "print \"Number of dev BC candidates without adj. boosting: \", len(cands_BC[1])\n",
    "add_adj_candidate_BC(session, BiomarkerCondition, cands_BC[1], 0)\n",
    "# fix_specificity(session, BiomarkerCondition, cands_BC[1])\n",
    "print \"Number of dev BC candidates with adj. boosting: \", session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 1).count()\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Functions\n",
    "\n",
    "The weak supervision portion of the pipeline, these labelling funtions are used to label the training data. In order to modify the accuracy of the pipeline, these should be modified, and new labelling functions should be added. More information about evaluating the accuracy of labelling functions can be found on the Snorkel website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LF import *\n",
    "LFs_BC = [LF_markerDatabase, LF_keyword, LF_distance, LF_abstract_titleWord, LF_single_letter,\n",
    "          LF_auxpass, LF_known_abs, LF_same_thing, LF_common_1000, LF_common_2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "BC_labeler = LabelAnnotator(lfs=LFs_BC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "%time L_train_BC = BC_labeler.apply(split=0)\n",
    "L_train_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time L_train_BC = BC_labeler.load_matrix(session, split=0)\n",
    "L_train_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_BC.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_BC.get_key(session, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train_BC, epochs=100, decay=0.95, step_size=0.1 / L_train_BC.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.weights.lf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train_BC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = BC_labeler.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import save_marginals\n",
    "%time save_marginals(session, L_train_BC, train_marginals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 0).order_by(BiomarkerCondition.id).all()\n",
    "dev_cands   = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 1).order_by(BiomarkerCondition.id).all()\n",
    "test_cands  = session.query(BiomarkerCondition).filter(BiomarkerCondition.split == 2).order_by(BiomarkerCondition.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "load_external_labels(session, BiomarkerCondition, 'Biomarker', 'Condition', 'articles/disease_gold_labels.tsv', dev_cands, annotator_name='gold')\n",
    "load_external_labels(session, BiomarkerCondition, 'Biomarker', 'Condition', 'articles/disease_gold_labels.tsv', test_cands, annotator_name='gold')\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(train_cands)\n",
    "print len(dev_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning.disc_models.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        50,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.25,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 100\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below information is generated using the test set as an accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = lstm.score(dev_cands, L_gold_test)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, dev_cands, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save_marginals(session, test_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lstm.predictions(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for prediction in predictions: \n",
    "    if(prediction == 1):\n",
    "        i+=1\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "while( i< len(train_cands)):\n",
    "    print(\"Candidate: {}. Prediction: {}\").format(train_cands[i], predictions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('biomarker')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
